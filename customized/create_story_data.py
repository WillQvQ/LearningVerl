#!/usr/bin/env python3
"""
ä¸­è‹±æ–‡å°è¯´ç¿»è¯‘æ•°æ®ç”Ÿæˆå™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
ä½¿ç”¨vLLMæ‰¹é‡ç”Ÿæˆ100æ¡åŒ…å«ä¸­æ–‡å°è¯´ã€è‹±æ–‡ç¿»è¯‘å’Œä¸­æ–‡æ‘˜è¦çš„æ•°æ®
æ”¯æŒç‰¹æ®Štokenå’Œå¤šåŒºåŸŸloss mask

æ•°æ®ç»“æ„ç‰¹ç‚¹ï¼š
- æ¯æ¡æ•°æ®åŒ…å«å›ºå®šçš„5ä¸ªæ®µè½ï¼šä¸­æ–‡æ­£æ–‡ -> <EN> -> è‹±æ–‡ç¿»è¯‘ -> </EN> -> ä¸­æ–‡æ‘˜è¦
- ç®€åŒ–ç»“æ„ï¼Œé¿å…é‡å¤çš„ä¸­è‹±æ–‡æ®µè½ç»„åˆ
- ç»Ÿä¸€çš„lossæƒé‡åˆ†é…ï¼šä¸­æ–‡(0.0) -> ç‰¹æ®Štoken(5.0) -> è‹±æ–‡(1.0) -> ç‰¹æ®Štoken(5.0) -> æ‘˜è¦(2.0)
"""

import sys
import os
import random
import json
import pandas as pd
from typing import List, Dict, Any
from tqdm import tqdm
import re

# æ·»åŠ è·¯å¾„
sys.path.append('/home/jovyan/fdu_new/zyn/verl')

# vLLMç›¸å…³å¯¼å…¥
try:
    from vllm import LLM, SamplingParams
    print("âœ… vLLMå¯¼å…¥æˆåŠŸ")
except ImportError:
    print("âŒ vLLMæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install vllm")
    sys.exit(1)


class ChineseNovelDataGenerator:
    """ä¸­æ–‡å°è¯´æ•°æ®ç”Ÿæˆå™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, model_path="/home/jovyan/fdu_new/models/Qwen2.5-7B-Instruct"):
        self.model_path = model_path
        self.special_tokens = ["<EN>", "</EN>"]
        
        # å°è¯´ä¸»é¢˜å’Œè®¾å®š
        self.novel_themes = [
            ("éƒ½å¸‚è¨€æƒ…", "ç°ä»£éƒ½å¸‚", "çˆ±æƒ…æ•…äº‹"),
            ("æ­¦ä¾ æ±Ÿæ¹–", "å¤ä»£æ±Ÿæ¹–", "æ­¦åŠŸç§˜ç±"),
            ("ä»™ä¾ ä¿®çœŸ", "ä¿®ä»™ä¸–ç•Œ", "æ³•å®ä¸¹è¯"),
            ("ç§‘å¹»æœªæ¥", "æœªæ¥ä¸–ç•Œ", "ç§‘æŠ€å‘å±•"),
            ("å†å²ç©¿è¶Š", "å¤ä»£å®«å»·", "æƒè°‹æ–—äº‰"),
            ("æ‚¬ç–‘æ¨ç†", "ç°ä»£ç¤¾ä¼š", "çŠ¯ç½ªä¾¦æ¢"),
            ("é’æ˜¥æ ¡å›­", "æ ¡å›­ç”Ÿæ´»", "é’æ˜¥æˆé•¿"),
            ("å•†æˆ˜èŒåœº", "å•†ä¸šä¸–ç•Œ", "èŒåœºç«äº‰"),
            ("å†›äº‹æˆ˜äº‰", "æˆ˜äº‰å¹´ä»£", "è‹±é›„äº‹è¿¹"),
            ("é­”å¹»å¥‡å¹»", "é­”æ³•ä¸–ç•Œ", "é­”æ³•å†’é™©")
        ]
        
        # åˆå§‹åŒ–vLLMæ¨¡å‹
        print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        try:
            self.llm = LLM(
                model=model_path,
                tensor_parallel_size=1,
                gpu_memory_utilization=0.8,
                max_model_len=4096,
                trust_remote_code=True
            )
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    
    def create_novel_prompt(self, sample_id: int) -> str:
        """åˆ›å»ºç”Ÿæˆå°è¯´çš„prompt"""
        theme, setting, element = random.choice(self.novel_themes)
        
        prompt = f"""è¯·åˆ›ä½œä¸€ä¸ª{theme}å°è¯´ç‰‡æ®µï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„ï¼š

1. èƒŒæ™¯è®¾å®šï¼š{setting}
2. æ ¸å¿ƒå…ƒç´ ï¼š{element}
3. è¦æ±‚ç»“æ„ï¼šä¸­æ–‡å°è¯´ + ä¸€æ¬¡è‹±æ–‡ç¿»è¯‘ + ä¸€ä¸ªä¸­æ–‡æ‘˜è¦

ä¸¥æ ¼æ ¼å¼è¦æ±‚ï¼š
ç¬¬ä¸€éƒ¨åˆ†ï¼šå†™ä¸€æ®µå®Œæ•´çš„ä¸­æ–‡å°è¯´å†…å®¹ï¼ˆ1500-2000å­—ï¼‰
ç¬¬äºŒéƒ¨åˆ†ï¼šå°†æ•´æ®µä¸­æ–‡å†…å®¹ç¿»è¯‘æˆè‹±æ–‡ï¼Œç”¨<EN>å’Œ</EN>æ ‡ç­¾åŒ…å›´ï¼ˆåªæœ‰ä¸€å¯¹æ ‡ç­¾ï¼‰
ç¬¬ä¸‰éƒ¨åˆ†ï¼šæä¾›ä¸€æ®µ150å­—çš„ä¸­æ–‡æ‘˜è¦ï¼Œä»¥"æ•…äº‹æ‘˜è¦ï¼š"å¼€å¤´

æ ¼å¼ç¤ºä¾‹ï¼š
ä¸»äººå…¬å¼ æ˜èµ°åœ¨å¤œæ™šçš„è¡—é“ä¸Šï¼Œå¿ƒæƒ…å¤æ‚ã€‚ä»–åˆšåˆšä»å…¬å¸è¾èŒï¼Œé¢ä¸´äººç”Ÿçš„é‡å¤§è½¬æŠ˜ã€‚è¡—ç¯ç…§äº®äº†ä»–çš„è„¸åºï¼Œä¹Ÿç…§äº®äº†å†…å¿ƒçš„è¿·èŒ«ã€‚ï¼ˆç»§ç»­1500å­—çš„å®Œæ•´æ•…äº‹...ï¼‰

<EN>The protagonist Zhang Ming walked down the night street with complicated feelings. He had just resigned from his company and was facing a major turning point in life. The street lights illuminated his face and also lit up the confusion in his heart. (Complete English translation of the entire story...)</EN>

æ•…äº‹æ‘˜è¦ï¼šä¸»äººå…¬å¼ æ˜åœ¨äººç”Ÿè½¬æŠ˜ç‚¹æ—¶é€‰æ‹©è¾èŒï¼Œé€šè¿‡å¤œæ™šçš„æ¼«æ­¥å’Œå†…å¿ƒç‹¬ç™½ï¼Œæœ€ç»ˆæ‰¾åˆ°äº†æ–°çš„äººç”Ÿæ–¹å‘ï¼Œä½“ç°äº†ç°ä»£éƒ½å¸‚äººé¢ä¸´é€‰æ‹©æ—¶çš„å‹‡æ°”ä¸æˆé•¿ã€‚

æ³¨æ„äº‹é¡¹ï¼š
- ä¸­æ–‡å°è¯´è¦å®Œæ•´è¿è´¯ï¼Œæœ‰å¼€å¤´ã€å‘å±•ã€é«˜æ½®ã€ç»“å°¾
- è‹±æ–‡ç¿»è¯‘è¦å‡†ç¡®ï¼Œè¯­æ³•æ­£ç¡®
- åªèƒ½æœ‰ä¸€å¯¹<EN></EN>æ ‡ç­¾
- æ‘˜è¦è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºä¸»é¢˜

è¯·æŒ‰æ­¤æ ¼å¼åˆ›ä½œï¼š"""

        return prompt
    
    def create_all_prompts(self, total_samples: int) -> List[str]:
        """åˆ›å»ºæ‰€æœ‰prompts"""
        print(f"ğŸ“ åˆ›å»º {total_samples} ä¸ªprompts...")
        prompts = []
        
        for i in range(total_samples):
            prompt = self.create_novel_prompt(i)
            prompts.append(prompt)
            
        print(f"âœ… åˆ›å»ºå®Œæˆ {len(prompts)} ä¸ªprompts")
        return prompts
    
    def generate_all_texts(self, prompts: List[str]) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆæ‰€æœ‰æ–‡æœ¬"""
        print(f"ğŸ¯ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {len(prompts)} ä¸ªæ–‡æœ¬...")
        
        # è®¾ç½®ç”Ÿæˆå‚æ•°
        sampling_params = SamplingParams(
            temperature=0.8,
            top_p=0.9,
            max_tokens=8192,
            stop=None
        )
        
        try:
            # æ‰¹é‡ç”Ÿæˆ
            outputs = self.llm.generate(prompts, sampling_params)
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_texts = []
            for output in outputs:
                generated_text = output.outputs[0].text.strip()
                generated_texts.append(generated_text)
            
            print(f"âœ… æ‰¹é‡ç”Ÿæˆå®Œæˆï¼Œå…± {len(generated_texts)} ä¸ªæ–‡æœ¬")
            return generated_texts
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›ç©ºåˆ—è¡¨ï¼Œåç»­ä½¿ç”¨å¤‡ç”¨æ ·æœ¬
            return []
    
    def parse_generated_content(self, text: str, sample_id: int) -> Dict[str, Any]:
        """è§£æç”Ÿæˆçš„å†…å®¹ï¼Œæå–å„ä¸ªéƒ¨åˆ†"""
        
        # å¯»æ‰¾æ‘˜è¦éƒ¨åˆ†ï¼ˆé€šå¸¸åœ¨æœ€åï¼‰
        summary_patterns = [
            r'æ‘˜è¦[:ï¼š]\s*(.+?)(?:\n|$)',
            r'æ€»ç»“[:ï¼š]\s*(.+?)(?:\n|$)', 
            r'ç®€ä»‹[:ï¼š]\s*(.+?)(?:\n|$)',
            r'(?:^|\n)([^<\n]{100,200}?[ã€‚ï¼ï¼Ÿ])(?:\n|$)'  # æœ€åä¸€ä¸ªé•¿å¥å­ä½œä¸ºæ‘˜è¦
        ]
        
        summary = ""
        main_text = text
        
        for pattern in summary_patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.MULTILINE)
            if matches:
                summary = matches[-1].strip()
                # ä»ä¸»æ–‡æœ¬ä¸­ç§»é™¤æ‘˜è¦
                main_text = re.sub(pattern, "", text, flags=re.DOTALL | re.MULTILINE)
                break
        
        if not summary:
            # å¦‚æœæ²¡æ‰¾åˆ°æ‘˜è¦ï¼Œå–æœ€åä¸€æ®µä½œä¸ºæ‘˜è¦
            paragraphs = [p.strip() for p in main_text.split('\n') if p.strip()]
            if paragraphs:
                summary = paragraphs[-1]
                main_text = '\n'.join(paragraphs[:-1])
        
        # æå–è‹±æ–‡éƒ¨åˆ† - åˆå¹¶æ‰€æœ‰è‹±æ–‡å†…å®¹
        english_parts = re.findall(r'<EN>(.*?)</EN>', main_text, re.DOTALL)
        
        # ç§»é™¤è‹±æ–‡éƒ¨åˆ†ï¼Œå¾—åˆ°çº¯ä¸­æ–‡éƒ¨åˆ†
        chinese_text = re.sub(r'<EN>.*?</EN>', "", main_text, flags=re.DOTALL)
        chinese_text = re.sub(r'\n+', '\n', chinese_text).strip()
        
        # åˆå¹¶æ‰€æœ‰è‹±æ–‡å†…å®¹ä¸ºä¸€æ®µ
        if english_parts:
            # åˆå¹¶å¤šä¸ªè‹±æ–‡æ®µè½ï¼Œç”¨ç©ºæ ¼è¿æ¥
            combined_english = " ".join([part.strip() for part in english_parts if part.strip()])
        else:
            # å¦‚æœæ²¡æœ‰è‹±æ–‡éƒ¨åˆ†ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ç¿»è¯‘
            combined_english = "The story continues with meaningful narrative and character development."
        
        # æ„å»ºç®€åŒ–çš„æ®µè½ç»“æ„ï¼šä¸­æ–‡ -> è‹±æ–‡ç¿»è¯‘ -> ä¸­æ–‡æ‘˜è¦
        segments = []
        segment_id = 0
        
        # 1. ä¸­æ–‡æ­£æ–‡ï¼ˆåˆå¹¶æ‰€æœ‰ä¸­æ–‡æ®µè½ï¼‰
        chinese_paragraphs = [p.strip() for p in chinese_text.split('\n') if p.strip()]
        if chinese_paragraphs:
            # åˆå¹¶ä¸­æ–‡æ®µè½ï¼Œä¿æŒåˆç†çš„é•¿åº¦
            combined_chinese = " ".join(chinese_paragraphs)
            # å¦‚æœå¤ªé•¿ï¼Œæˆªå–å‰2000å­—ç¬¦
            if len(combined_chinese) > 2000:
                combined_chinese = combined_chinese[:2000] + "..."
                
            segments.append({
                "content": combined_chinese,
                "learn": False,  # ä¸­æ–‡ä¸åŠ loss
                "weight": 0.0,
                "segment_type": "chinese_text",
                "segment_id": segment_id
            })
            segment_id += 1
        
        # 2. è‹±æ–‡ç¿»è¯‘éƒ¨åˆ†ï¼ˆåªæœ‰ä¸€æ¬¡ï¼‰
        # ç‰¹æ®Štokenå¼€å§‹
        segments.append({
            "content": "<EN>",
            "learn": True,
            "weight": 5.0,  # ç‰¹æ®Štoken 5å€loss
            "segment_type": "special_token_start", 
            "segment_id": segment_id
        })
        segment_id += 1
        
        # è‹±æ–‡ç¿»è¯‘å†…å®¹
        segments.append({
            "content": combined_english,
            "learn": True,
            "weight": 1.0,  # è‹±æ–‡ç¿»è¯‘ 1å€loss
            "segment_type": "english_translation",
            "segment_id": segment_id
        })
        segment_id += 1
        
        # ç‰¹æ®Štokenç»“æŸ
        segments.append({
            "content": "</EN>",
            "learn": True,
            "weight": 5.0,  # ç‰¹æ®Štoken 5å€loss
            "segment_type": "special_token_end",
            "segment_id": segment_id
        })
        segment_id += 1
        
        # 3. ä¸­æ–‡æ‘˜è¦ï¼ˆåªæœ‰ä¸€æ¬¡ï¼‰
        if summary:
            segments.append({
                "content": f"æ•…äº‹æ‘˜è¦ï¼š{summary}",
                "learn": True,
                "weight": 2.0,  # ä¸­æ–‡æ‘˜è¦ 2å€loss
                "segment_type": "chinese_summary",
                "segment_id": segment_id
            })
        
        return {
            "format": "structured",
            "data": {"segments": segments},
            "id": sample_id,
            "topic": "ä¸­è‹±æ–‡å°è¯´ç¿»è¯‘",
            "total_segments": len(segments),
            "english_segments": len([s for s in segments if s["segment_type"] == "english_translation"]),
            "special_tokens": len([s for s in segments if "special_token" in s["segment_type"]])
        }
    
    def create_fallback_sample(self, sample_id: int) -> Dict[str, Any]:
        """åˆ›å»ºå¤‡ç”¨æ ·æœ¬ï¼ˆå½“ç”Ÿæˆå¤±è´¥æ—¶ï¼‰"""
        # éšæœºé€‰æ‹©ä¸»é¢˜åˆ›å»ºå¤šæ ·åŒ–çš„å¤‡ç”¨æ ·æœ¬
        theme, setting, element = random.choice(self.novel_themes)
        
        fallback_stories = [
            {
                "chinese": f"åœ¨{setting}ä¸­ï¼Œä¸»è§’é¢ä¸´ç€å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ã€‚",
                "english": f"In the world of {element.lower()}, the protagonist faces unprecedented challenges.",
                "summary": f"ä¸€ä¸ªå…³äº{theme}çš„æ•…äº‹ï¼Œè®²è¿°äº†{element}å¸¦æ¥çš„å˜åŒ–ã€‚"
            },
            {
                "chinese": f"æœˆå…‰æ´’åœ¨{setting}çš„è¡—é“ä¸Šï¼Œä¸€åˆ‡éƒ½æ˜¾å¾—æ ¼å¤–å®é™ã€‚",
                "english": "The moonlight cast a serene glow over the ancient streets, whispering tales of old.",
                "summary": f"æè¿°{setting}å¤œæ™šçš„å®é™ä¸ç¾å¥½ï¼Œä½“ç°{theme}çš„ç‹¬ç‰¹é­…åŠ›ã€‚"
            },
            {
                "chinese": f"ä»–ç¼“ç¼“æ¨å¼€é—¨ï¼Œ{element}çš„æ°”æ¯æ‰‘é¢è€Œæ¥ã€‚",
                "english": f"As he slowly opened the door, the essence of {element.lower()} filled the air.",
                "summary": f"é€šè¿‡å¼€é—¨è¿™ä¸€åŠ¨ä½œï¼Œå±•ç°{theme}ä¸–ç•Œçš„ç¥ç§˜ä¸é­…åŠ›ã€‚"
            }
        ]
        
        story = random.choice(fallback_stories)
        
        segments = [
            {
                "content": story["chinese"],
                "learn": False,
                "weight": 0.0,
                "segment_type": "chinese_text",
                "segment_id": 0
            },
            {
                "content": "<EN>",
                "learn": True,
                "weight": 5.0,
                "segment_type": "special_token_start",
                "segment_id": 1
            },
            {
                "content": story["english"],
                "learn": True,
                "weight": 1.0,
                "segment_type": "english_translation",
                "segment_id": 2
            },
            {
                "content": "</EN>",
                "learn": True,
                "weight": 5.0,
                "segment_type": "special_token_end", 
                "segment_id": 3
            },
            {
                "content": f"æ•…äº‹æ‘˜è¦ï¼š{story['summary']}",
                "learn": True,
                "weight": 2.0,
                "segment_type": "chinese_summary",
                "segment_id": 4
            }
        ]
        
        return {
            "format": "structured",
            "data": {"segments": segments},
            "id": sample_id,
            "topic": "ä¸­è‹±æ–‡å°è¯´ç¿»è¯‘",
            "total_segments": len(segments),
            "english_segments": 1,
            "special_tokens": 2
        }
    
    def process_all_texts(self, generated_texts: List[str], total_samples: int) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†æ‰€æœ‰ç”Ÿæˆçš„æ–‡æœ¬"""
        print(f"ğŸ”„ å¼€å§‹å¤„ç† {len(generated_texts)} ä¸ªç”Ÿæˆæ–‡æœ¬...")
        
        all_samples = []
        
        # å¤„ç†æˆåŠŸç”Ÿæˆçš„æ–‡æœ¬
        for i, text in enumerate(tqdm(generated_texts, desc="å¤„ç†ç”Ÿæˆæ–‡æœ¬")):
            if text and text.strip():
                try:
                    sample = self.parse_generated_content(text, i)
                    all_samples.append(sample)
                except Exception as e:
                    print(f"âš ï¸  å¤„ç†ç¬¬{i}ä¸ªæ–‡æœ¬æ—¶å‡ºé”™: {e}")
                    fallback_sample = self.create_fallback_sample(i)
                    all_samples.append(fallback_sample)
            else:
                # æ–‡æœ¬ä¸ºç©ºï¼Œä½¿ç”¨å¤‡ç”¨æ ·æœ¬
                fallback_sample = self.create_fallback_sample(i)
                all_samples.append(fallback_sample)
        
        # å¦‚æœç”Ÿæˆçš„æ–‡æœ¬ä¸å¤Ÿï¼Œè¡¥å……å¤‡ç”¨æ ·æœ¬
        while len(all_samples) < total_samples:
            sample_id = len(all_samples)
            fallback_sample = self.create_fallback_sample(sample_id)
            all_samples.append(fallback_sample)
            print(f"  â• æ·»åŠ å¤‡ç”¨æ ·æœ¬ {sample_id}")
        
        print(f"âœ… å¤„ç†å®Œæˆï¼Œå…± {len(all_samples)} ä¸ªæ ·æœ¬")
        return all_samples
    
    def generate_all_data(self, total_samples: int = 1000) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ‰€æœ‰æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆ {total_samples} ä¸ªä¸­è‹±æ–‡å°è¯´æ ·æœ¬...")
        print("=" * 60)
        
        # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ‰€æœ‰prompts
        prompts = self.create_all_prompts(total_samples)
        
        # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡ç”Ÿæˆæ‰€æœ‰æ–‡æœ¬
        generated_texts = self.generate_all_texts(prompts)
        
        # ç¬¬ä¸‰æ­¥ï¼šæ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡æœ¬
        all_samples = self.process_all_texts(generated_texts, total_samples)
        
        print(f"\nâœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼æ€»è®¡: {len(all_samples)} ä¸ªæ ·æœ¬")
        return all_samples


def create_tokenizer_with_special_tokens():
    """åˆ›å»ºåŒ…å«ç‰¹æ®Štokençš„tokenizeré…ç½®"""
    
    tokenizer_config = {
        "special_tokens": {
            "additional_special_tokens": ["<EN>", "</EN>"]
        },
        "usage_instructions": """
ä½¿ç”¨æ–¹æ³•ï¼š
1. åŠ è½½tokenizeråæ·»åŠ ç‰¹æ®Štokensï¼š
   tokenizer.add_special_tokens({'additional_special_tokens': ['<EN>', '</EN>']})
   
2. è°ƒæ•´æ¨¡å‹embeddingå±‚å¤§å°ï¼š
   model.resize_token_embeddings(len(tokenizer))
   
3. ç‰¹æ®Štokençš„IDï¼š
   en_start_id = tokenizer.convert_tokens_to_ids('<EN>')
   en_end_id = tokenizer.convert_tokens_to_ids('</EN>')
"""
    }
    
    return tokenizer_config


def analyze_generated_data(samples: List[Dict[str, Any]]) -> Dict[str, Any]:
    """åˆ†æç”Ÿæˆçš„æ•°æ®"""
    
    analysis = {
        "total_samples": len(samples),
        "total_segments": 0,
        "segment_types": {},
        "weight_distribution": {},
        "learning_segments": 0,
        "english_segments": 0,
        "special_tokens": 0,
        "average_segments_per_sample": 0
    }
    
    for sample in samples:
        segments = sample["data"]["segments"]
        analysis["total_segments"] += len(segments)
        
        for segment in segments:
            # ç»Ÿè®¡æ®µè½ç±»å‹
            seg_type = segment["segment_type"]
            analysis["segment_types"][seg_type] = analysis["segment_types"].get(seg_type, 0) + 1
            
            # ç»Ÿè®¡æƒé‡åˆ†å¸ƒ
            weight = segment["weight"]
            weight_key = f"weight_{weight}"
            analysis["weight_distribution"][weight_key] = analysis["weight_distribution"].get(weight_key, 0) + 1
            
            # ç»Ÿè®¡å­¦ä¹ æ®µè½
            if segment["learn"]:
                analysis["learning_segments"] += 1
                
                if seg_type == "english_translation":
                    analysis["english_segments"] += 1
                elif "special_token" in seg_type:
                    analysis["special_tokens"] += 1
    
    analysis["average_segments_per_sample"] = analysis["total_segments"] / len(samples) if len(samples) > 0 else 0
    analysis["learning_ratio"] = analysis["learning_segments"] / analysis["total_segments"] if analysis["total_segments"] > 0 else 0
    
    return analysis


def save_data_and_configs(samples: List[Dict[str, Any]], output_dir: str):
    """ä¿å­˜æ•°æ®å’Œé…ç½®æ–‡ä»¶"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(samples)
    
    # æŒ‰80/20æ¯”ä¾‹åˆ†å‰²
    train_size = int(0.8 * len(df))
    train_df = df[:train_size]
    test_df = df[train_size:]
    
    # ä¿å­˜parquetæ–‡ä»¶
    train_path = os.path.join(output_dir, "train.parquet")
    test_path = os.path.join(output_dir, "test.parquet")
    
    train_df.to_parquet(train_path, index=False)
    test_df.to_parquet(test_path, index=False)
    
    # ä¿å­˜tokenizeré…ç½®
    tokenizer_config = create_tokenizer_with_special_tokens()
    tokenizer_config_path = os.path.join(output_dir, "tokenizer_config.json")
    
    with open(tokenizer_config_path, 'w', encoding='utf-8') as f:
        json.dump(tokenizer_config, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜æ•°æ®åˆ†ææŠ¥å‘Š
    analysis = analyze_generated_data(samples)
    analysis_path = os.path.join(output_dir, "data_analysis.json")
    
    with open(analysis_path, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    
    return train_path, test_path, tokenizer_config_path, analysis_path


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¸­è‹±æ–‡å°è¯´ç¿»è¯‘æ•°æ®ç”Ÿæˆå™¨ï¼ˆå•ä¸€ç»“æ„ç‰ˆï¼‰")
    print("=" * 60)
    print("åŠŸèƒ½ï¼šæ‰¹é‡ç”Ÿæˆ100æ¡åŒ…å«ä¸­æ–‡å°è¯´ã€è‹±æ–‡ç¿»è¯‘å’Œä¸­æ–‡æ‘˜è¦çš„æ•°æ®")
    print("ç‰¹è‰²ï¼šæ”¯æŒç‰¹æ®Štoken <EN> </EN> å’Œå¤šåŒºåŸŸlossæ§åˆ¶")
    print("ç»“æ„ï¼šæ¯æ¡æ•°æ®å›ºå®š5ä¸ªæ®µè½ - ä¸­æ–‡->ç‰¹æ®Štoken->è‹±æ–‡->ç‰¹æ®Štoken->æ‘˜è¦")
    print("ä¼˜åŒ–ï¼šæ‰¹é‡ç”Ÿæˆæå‡æ•ˆç‡ï¼Œä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰prompts")
    print("=" * 60)
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = ChineseNovelDataGenerator()
    
    # ç”Ÿæˆæ•°æ®
    samples = generator.generate_all_data(total_samples=2000)
    
    # ä¿å­˜æ•°æ®å’Œé…ç½®
    output_dir = "/home/jovyan/fdu_new/zyn/examples"
    train_path, test_path, tokenizer_config_path, analysis_path = save_data_and_configs(samples, output_dir)
    
    # åˆ†æç»“æœ
    analysis = analyze_generated_data(samples)
    
    print(f"\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®ç”Ÿæˆå’Œåˆ†æå®Œæˆ")
    print("=" * 60)
    print(f"è®­ç»ƒé›†: {len(pd.read_parquet(train_path))} æ ·æœ¬ -> {train_path}")
    print(f"æµ‹è¯•é›†: {len(pd.read_parquet(test_path))} æ ·æœ¬ -> {test_path}")
    print(f"Tokenizeré…ç½®: {tokenizer_config_path}")
    print(f"æ•°æ®åˆ†ææŠ¥å‘Š: {analysis_path}")
    
    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {analysis['total_samples']}")
    print(f"  æ€»æ®µè½æ•°: {analysis['total_segments']}")
    print(f"  å­¦ä¹ æ®µè½æ•°: {analysis['learning_segments']}")
    print(f"  è‹±æ–‡ç¿»è¯‘æ®µè½: {analysis['english_segments']}")
    print(f"  ç‰¹æ®Štokenæ®µè½: {analysis['special_tokens']}")
    print(f"  å¹³å‡æ®µè½/æ ·æœ¬: {analysis['average_segments_per_sample']:.1f}")
    print(f"  å­¦ä¹ æ®µè½æ¯”ä¾‹: {analysis['learning_ratio']:.2%}")
    
    print(f"\nğŸ¯ Lossæƒé‡åˆ†å¸ƒ:")
    for weight_key, count in sorted(analysis['weight_distribution'].items()):
        print(f"  {weight_key}: {count} ä¸ªæ®µè½")
    
    print(f"\nğŸ“ æ®µè½ç±»å‹åˆ†å¸ƒ:")
    for seg_type, count in sorted(analysis['segment_types'].items(), key=lambda x: x[1], reverse=True):
        print(f"  {seg_type}: {count} ä¸ªæ®µè½")
    
    print(f"\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒäº†ã€‚")
    print("ğŸ’¡ ç»“æ„ä¼˜åŒ–ï¼šæ¯æ¡æ•°æ®å›ºå®šä¸º ä¸­æ–‡-è‹±æ–‡-æ‘˜è¦ å•ä¸€ç»„åˆ")
    print("ğŸ¯ æ•°æ®ç‰¹ç‚¹ï¼šç®€åŒ–ç»“æ„ï¼Œç»Ÿä¸€æƒé‡ï¼Œæ˜“äºè®­ç»ƒå’Œè¯„ä¼°")
    print("=" * 60)


if __name__ == "__main__":
    main()