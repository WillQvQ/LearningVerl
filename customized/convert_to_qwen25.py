#!/usr/bin/env python3
"""
Qwen2.5 æ•°æ®æ ¼å¼è½¬æ¢å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
å°†create_new_data.pyç”Ÿæˆçš„æ•°æ®è½¬æ¢ä¸ºQwen2.5æ¨¡æ¿æ ¼å¼
ä¿®æ”¹ï¼šchinese_text éƒ¨åˆ†ä¹Ÿå‚ä¸ loss è®¡ç®—
æ–°å¢ï¼šspecial token ç»Ÿè®¡åˆ†æå’Œæ•°æ®è¿‡æ»¤
"""

import os
import json
import pandas as pd
from typing import List, Dict, Any
from tqdm import tqdm
import re

class Qwen25DataConverter:
    """Qwen2.5æ•°æ®æ ¼å¼è½¬æ¢å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        # Qwen2.5å®˜æ–¹system prompt
        self.system_prompt = "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."
        
        # å›ºå®šçš„ç”¨æˆ·query
        self.user_query = "è¯·ä½ å¸®æˆ‘å†™ä¸€ä¸ª 4000 å­—çš„å°è¯´ï¼Œå¹¶ç¿»è¯‘æˆè‹±æ–‡"
        
        # Qwen2.5æ¨¡æ¿æ ¼å¼
        self.qwen_template = {
            "system_start": "<|im_start|>system\n",
            "system_end": "<|im_end|>\n",
            "user_start": "<|im_start|>user\n", 
            "user_end": "<|im_end|>\n",
            "assistant_start": "<|im_start|>assistant\n",
            "assistant_end": "<|im_end|>"
        }
        
        # ç‰¹æ®Štokenåˆ—è¡¨ï¼ˆç”¨äºç»Ÿè®¡å’Œè¿‡æ»¤ï¼‰
        self.special_tokens = {
            "<|im_start|>": "qwen_template",
            "<|im_end|>": "qwen_template",
            "<EN>": "custom_special",
            "</EN>": "custom_special",
            "<ZH>": "custom_special",
            "</ZH>": "custom_special",
            "<NOVEL>": "custom_special", 
            "</NOVEL>": "custom_special",
            "<SUMMARY>": "custom_special",
            "</SUMMARY>": "custom_special"
        }
    
    def load_original_data(self, data_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½åŸå§‹æ•°æ®"""
        print(f"ğŸ“– åŠ è½½åŸå§‹æ•°æ®: {data_path}")
        
        if data_path.endswith('.parquet'):
            df = pd.read_parquet(data_path)
            data = df.to_dict('records')
        elif data_path.endswith('.json'):
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        else:
            raise ValueError("æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .parquet æˆ– .json")
        
        print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(data)} ä¸ªæ ·æœ¬")
        return data
    
    def analyze_special_tokens_in_content(self, content: str) -> Dict[str, Any]:
        """åˆ†æå†…å®¹ä¸­çš„ç‰¹æ®Štoken"""
        token_stats = {}
        
        for token, token_type in self.special_tokens.items():
            count = content.count(token)
            if count > 0:
                if token_type not in token_stats:
                    token_stats[token_type] = {}
                token_stats[token_type][token] = count
        
        # æ£€æŸ¥ç‰¹æ®Štokenå¯¹æ˜¯å¦å¹³è¡¡
        balance_check = {}
        token_pairs = [
            ("<EN>", "</EN>"),
            ("<ZH>", "</ZH>"),
            ("<NOVEL>", "</NOVEL>"),
            ("<SUMMARY>", "</SUMMARY>")
        ]
        
        for start_token, end_token in token_pairs:
            start_count = content.count(start_token)
            end_count = content.count(end_token)
            if start_count > 0 or end_count > 0:
                balance_check[f"{start_token}_{end_token}"] = {
                    "start_count": start_count,
                    "end_count": end_count,
                    "balanced": start_count == end_count,
                    "difference": abs(start_count - end_count)
                }
        
        return {
            "token_stats": token_stats,
            "balance_check": balance_check,
            "total_special_tokens": sum(
                sum(tokens.values()) for tokens in token_stats.values()
            )
        }
    
    def analyze_en_token_pairing(self, content: str, sample_id: str = "unknown") -> Dict[str, Any]:
        """è¯¦ç»†åˆ†æ<EN>å’Œ</EN>çš„æˆå¯¹æƒ…å†µ"""
        en_start_count = content.count("<EN>")
        en_end_count = content.count("</EN>")
        
        # åŸºæœ¬ç»Ÿè®¡
        pairing_analysis = {
            "sample_id": sample_id,
            "en_start_count": en_start_count,
            "en_end_count": en_end_count,
            "is_balanced": en_start_count == en_end_count,
            "difference": abs(en_start_count - en_end_count),
            "pairing_status": "unknown",
            "valid_pairs": 0,
            "orphaned_starts": 0,
            "orphaned_ends": 0
        }
        
        # ç¡®å®šé…å¯¹çŠ¶æ€
        if en_start_count == 0 and en_end_count == 0:
            pairing_analysis["pairing_status"] = "no_tokens"
        elif en_start_count == 1 and en_end_count == 1:
            pairing_analysis["pairing_status"] = "perfect_pair"
            pairing_analysis["valid_pairs"] = 1
        elif en_start_count == en_end_count and en_start_count > 1:
            pairing_analysis["pairing_status"] = "multiple_balanced"
            pairing_analysis["valid_pairs"] = en_start_count
        elif en_start_count > en_end_count:
            pairing_analysis["pairing_status"] = "excess_starts"
            pairing_analysis["valid_pairs"] = en_end_count
            pairing_analysis["orphaned_starts"] = en_start_count - en_end_count
        elif en_end_count > en_start_count:
            pairing_analysis["pairing_status"] = "excess_ends"
            pairing_analysis["valid_pairs"] = en_start_count
            pairing_analysis["orphaned_ends"] = en_end_count - en_start_count
        else:
            pairing_analysis["pairing_status"] = "unknown_error"
        
        # æ£€æŸ¥æ˜¯å¦ç¬¦åˆæˆ‘ä»¬çš„ç­›é€‰æ¡ä»¶
        pairing_analysis["passes_filter"] = pairing_analysis["pairing_status"] in ["no_tokens", "perfect_pair"]
        
        return pairing_analysis
    
    def is_valid_sample(self, sample: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ ·æœ¬æ˜¯å¦æœ‰æ•ˆï¼ˆç‰¹æ®Štokenå¹³è¡¡ä¸”æ•°é‡æ­£ç¡®ï¼‰"""
        # æå–åŠ©æ‰‹å›å¤
        assistant_response = self.extract_assistant_response(sample)
        
        # åˆ†æ<EN></EN>é…å¯¹æƒ…å†µ
        pairing_analysis = self.analyze_en_token_pairing(assistant_response, sample.get("id", "unknown"))
        
        # åªæ¥å—æ— tokenæˆ–å®Œç¾é…å¯¹çš„æ ·æœ¬
        return pairing_analysis["passes_filter"]
    
    def extract_assistant_response(self, sample: Dict[str, Any]) -> str:
        """ä»åŸå§‹æ ·æœ¬ä¸­æå–åŠ©æ‰‹å›å¤å†…å®¹"""
        segments = sample["data"]["segments"]
        
        # æŒ‰é¡ºåºç»„åˆæ‰€æœ‰å†…å®¹
        response_parts = []
        for segment in segments:
            content = segment["content"]
            response_parts.append(content)
        
        # ç”¨æ¢è¡Œç¬¦è¿æ¥æ‰€æœ‰éƒ¨åˆ†
        assistant_response = "\n\n".join(response_parts)
        return assistant_response
    
    def create_qwen25_conversation(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºQwen2.5æ ¼å¼çš„å¯¹è¯"""
        
        # æå–åŠ©æ‰‹å›å¤
        assistant_response = self.extract_assistant_response(sample)
        
        # åˆ†æåŠ©æ‰‹å›å¤ä¸­çš„ç‰¹æ®Štoken
        special_token_analysis = self.analyze_special_tokens_in_content(assistant_response)
        
        # åˆ†æ<EN></EN>é…å¯¹æƒ…å†µ
        en_pairing_analysis = self.analyze_en_token_pairing(assistant_response, sample.get("id", "unknown"))
        
        # æ„å»ºå®Œæ•´çš„å¯¹è¯æ–‡æœ¬
        conversation = (
            f"{self.qwen_template['system_start']}"
            f"{self.system_prompt}"
            f"{self.qwen_template['system_end']}"
            f"{self.qwen_template['user_start']}"
            f"{self.user_query}"
            f"{self.qwen_template['user_end']}"
            f"{self.qwen_template['assistant_start']}"
            f"{assistant_response}"
            f"{self.qwen_template['assistant_end']}"
        )
        
        # åˆ†æå®Œæ•´å¯¹è¯ä¸­çš„ç‰¹æ®Štoken
        full_conversation_analysis = self.analyze_special_tokens_in_content(conversation)
        
        # åˆ›å»ºloss mask - ä¿®æ”¹ç­–ç•¥ï¼šchinese_textä¹Ÿè®¡ç®—loss
        segments = sample["data"]["segments"]
        loss_segments = []
        
        # æ·»åŠ systeméƒ¨åˆ† - ä¸è®¡ç®—loss
        system_text = (
            f"{self.qwen_template['system_start']}"
            f"{self.system_prompt}"
            f"{self.qwen_template['system_end']}"
        )
        loss_segments.append({
            "content": system_text,
            "learn": False,
            "weight": 0.0,
            "segment_type": "system_prompt",
            "segment_id": 0,
            "special_tokens": self.analyze_special_tokens_in_content(system_text)
        })
        
        # æ·»åŠ useréƒ¨åˆ† - ä¸è®¡ç®—loss  
        user_text = (
            f"{self.qwen_template['user_start']}"
            f"{self.user_query}"
            f"{self.qwen_template['user_end']}"
        )
        loss_segments.append({
            "content": user_text,
            "learn": False,
            "weight": 0.0,
            "segment_type": "user_query",
            "segment_id": 1,
            "special_tokens": self.analyze_special_tokens_in_content(user_text)
        })
        
        # æ·»åŠ assistantå¼€å§‹æ ‡ç­¾ - ä¸è®¡ç®—loss
        loss_segments.append({
            "content": self.qwen_template['assistant_start'],
            "learn": False,
            "weight": 0.0,
            "segment_type": "assistant_start_tag",
            "segment_id": 2,
            "special_tokens": self.analyze_special_tokens_in_content(self.qwen_template['assistant_start'])
        })
        
        # æ·»åŠ assistantå†…å®¹éƒ¨åˆ† - ä¿®æ”¹ï¼šchinese_textä¹Ÿå‚ä¸å­¦ä¹ 
        segment_id = 3
        for segment in segments:
            original_learn = segment["learn"]
            original_weight = segment["weight"]
            segment_type = segment["segment_type"]
            
            # åˆ†ææ¯ä¸ªæ®µè½çš„ç‰¹æ®Štoken
            segment_token_analysis = self.analyze_special_tokens_in_content(segment["content"])
            
            # ğŸ”¥ ä¿®æ”¹ï¼šå¦‚æœæ˜¯chinese_textç±»å‹ï¼Œå¼ºåˆ¶è®¾ç½®ä¸ºå­¦ä¹ çŠ¶æ€
            if segment_type == "chinese_text":
                new_learn = True
                new_weight = 1.0 if original_weight == 0.0 else original_weight
            else:
                new_learn = original_learn
                new_weight = original_weight
            
            loss_segments.append({
                "content": segment["content"],
                "learn": new_learn,
                "weight": new_weight,
                "segment_type": segment_type,
                "segment_id": segment_id,
                "original_learn": original_learn,
                "original_weight": original_weight,
                "special_tokens": segment_token_analysis
            })
            segment_id += 1
        
        # æ·»åŠ assistantç»“æŸæ ‡ç­¾ - å­¦ä¹ 
        loss_segments.append({
            "content": self.qwen_template['assistant_end'],
            "learn": True,
            "weight": 1.0,
            "segment_type": "assistant_end_tag", 
            "segment_id": segment_id,
            "special_tokens": self.analyze_special_tokens_in_content(self.qwen_template['assistant_end'])
        })
        
        return {
            "format": "qwen25_structured",
            "conversation": conversation,
            "data": {"segments": loss_segments},
            "id": sample["id"],
            "topic": sample["topic"],
            "template": "qwen25",
            "total_segments": len(loss_segments),
            "learning_segments": len([s for s in loss_segments if s["learn"]]),
            "original_segments": sample["total_segments"],
            "chinese_text_modified": True,
            "special_token_analysis": {
                "assistant_response": special_token_analysis,
                "full_conversation": full_conversation_analysis
            },
            "en_pairing_analysis": en_pairing_analysis  # æ–°å¢ï¼šENé…å¯¹åˆ†æ
        }
    
    def convert_batch(self, samples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡è½¬æ¢æ ·æœ¬ï¼ˆåŒ…å«æ•°æ®è¿‡æ»¤ï¼‰"""
        print(f"ğŸ”„ å¼€å§‹è½¬æ¢ {len(samples)} ä¸ªæ ·æœ¬ä¸ºQwen2.5æ ¼å¼...")
        print("ğŸ’¡ æ–°ç­–ç•¥ï¼šchinese_text éƒ¨åˆ†ä¹Ÿå‚ä¸ loss è®¡ç®—")
        print("ğŸ” æ–°åŠŸèƒ½ï¼šè¿‡æ»¤ä¸åˆæ ¼çš„ç‰¹æ®Štokenæ ·æœ¬")
        
        # é¦–å…ˆç»Ÿè®¡æ‰€æœ‰æ ·æœ¬çš„<EN></EN>é…å¯¹æƒ…å†µ
        print("ğŸ” ç¬¬ä¸€æ­¥ï¼šç»Ÿè®¡<EN></EN>é…å¯¹æƒ…å†µ...")
        pairing_statistics = {
            "no_tokens": 0,
            "perfect_pair": 0,
            "multiple_balanced": 0,
            "excess_starts": 0,
            "excess_ends": 0,
            "unknown_error": 0
        }
        
        all_pairing_analyses = []
        for sample in tqdm(samples, desc="é…å¯¹åˆ†æ"):
            assistant_response = self.extract_assistant_response(sample)
            pairing_analysis = self.analyze_en_token_pairing(assistant_response, sample.get("id", "unknown"))
            all_pairing_analyses.append(pairing_analysis)
            pairing_statistics[pairing_analysis["pairing_status"]] += 1
        
        # æ˜¾ç¤ºé…å¯¹ç»Ÿè®¡
        print(f"\nğŸ“Š <EN></EN>é…å¯¹ç»Ÿè®¡:")
        print(f"  æ— tokenæ ·æœ¬: {pairing_statistics['no_tokens']} ({pairing_statistics['no_tokens']/len(samples)*100:.1f}%)")
        print(f"  å®Œç¾é…å¯¹(1å¯¹): {pairing_statistics['perfect_pair']} ({pairing_statistics['perfect_pair']/len(samples)*100:.1f}%)")
        print(f"  å¤šå¯¹å¹³è¡¡: {pairing_statistics['multiple_balanced']} ({pairing_statistics['multiple_balanced']/len(samples)*100:.1f}%)")
        print(f"  å¼€å§‹æ ‡ç­¾è¿‡å¤š: {pairing_statistics['excess_starts']} ({pairing_statistics['excess_starts']/len(samples)*100:.1f}%)")
        print(f"  ç»“æŸæ ‡ç­¾è¿‡å¤š: {pairing_statistics['excess_ends']} ({pairing_statistics['excess_ends']/len(samples)*100:.1f}%)")
        print(f"  æœªçŸ¥é”™è¯¯: {pairing_statistics['unknown_error']} ({pairing_statistics['unknown_error']/len(samples)*100:.1f}%)")
        
        # è®¡ç®—é€šè¿‡ç­›é€‰çš„æ ·æœ¬æ•°é‡
        valid_count = pairing_statistics['no_tokens'] + pairing_statistics['perfect_pair']
        invalid_count = len(samples) - valid_count
        print(f"\nâœ… ç­›é€‰ç»“æœ: {valid_count}/{len(samples)} ä¸ªæ ·æœ¬é€šè¿‡ç­›é€‰, å°†åˆ é™¤ {invalid_count} ä¸ªæ ·æœ¬")
        
        # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬
        valid_samples = []
        print("ğŸ” ç¬¬äºŒæ­¥ï¼šè¿‡æ»¤æ— æ•ˆæ ·æœ¬...")
        for sample in tqdm(samples, desc="è¿‡æ»¤è¿›åº¦"):
            if self.is_valid_sample(sample):
                valid_samples.append(sample)
        
        print(f"âœ… è¿‡æ»¤å®Œæˆ: æœ‰æ•ˆæ ·æœ¬ {len(valid_samples)}/{len(samples)}")
        
        # è½¬æ¢æœ‰æ•ˆæ ·æœ¬
        converted_samples = []
        chinese_text_modifications = 0
        
        print("ğŸ”„ ç¬¬ä¸‰æ­¥ï¼šè½¬æ¢æœ‰æ•ˆæ ·æœ¬...")
        for sample in tqdm(valid_samples, desc="è½¬æ¢è¿›åº¦"):
            try:
                qwen_sample = self.create_qwen25_conversation(sample)
                converted_samples.append(qwen_sample)
                
                # ç»Ÿè®¡chinese_textä¿®æ”¹æ¬¡æ•°
                if qwen_sample.get("chinese_text_modified", False):
                    chinese_text_modifications += 1
                    
            except Exception as e:
                print(f"âš ï¸  è½¬æ¢æ ·æœ¬ {sample.get('id', 'unknown')} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"âœ… è½¬æ¢å®Œæˆï¼Œå…± {len(converted_samples)} ä¸ªæ ·æœ¬")
        print(f"ğŸ”„ chinese_text ä¿®æ”¹æ¬¡æ•°: {chinese_text_modifications}")
        
        # å°†é…å¯¹ç»Ÿè®¡ä¿¡æ¯é™„åŠ åˆ°ç»“æœä¸­
        return converted_samples, pairing_statistics
    
    def analyze_converted_data(self, samples: List[Dict[str, Any]], pairing_stats: Dict[str, int] = None) -> Dict[str, Any]:
        """åˆ†æè½¬æ¢åçš„æ•°æ®"""
        
        analysis = {
            "total_samples": len(samples),
            "total_segments": 0,
            "learning_segments": 0,
            "segment_types": {},
            "weight_distribution": {},
            "template_overhead": 0,
            "content_segments": 0,
            "chinese_text_stats": {
                "total_chinese_text_segments": 0,
                "learning_chinese_text_segments": 0,
                "modified_chinese_text_segments": 0
            },
            "special_token_stats": {
                "samples_with_tokens": 0,
                "total_token_instances": 0,
                "token_type_distribution": {},
                "specific_token_counts": {},
                "balance_issues": {
                    "total_balance_issues": 0,
                    "issue_details": {}
                },
                "segments_with_tokens": 0
            },
            "en_pairing_stats": {  # æ–°å¢ï¼šENé…å¯¹ç»Ÿè®¡
                "converted_samples_pairing": {
                    "no_tokens": 0,
                    "perfect_pair": 0,
                    "multiple_balanced": 0,
                    "excess_starts": 0,
                    "excess_ends": 0,
                    "unknown_error": 0
                },
                "original_samples_pairing": pairing_stats if pairing_stats else {},
                "filtering_efficiency": 0.0
            }
        }
        
        # ç»Ÿè®¡è½¬æ¢åæ ·æœ¬çš„ENé…å¯¹æƒ…å†µ
        for sample in samples:
            segments = sample["data"]["segments"]
            analysis["total_segments"] += len(segments)
            
            # ç»Ÿè®¡è½¬æ¢åæ ·æœ¬çš„ENé…å¯¹çŠ¶æ€
            en_pairing = sample.get("en_pairing_analysis", {})
            pairing_status = en_pairing.get("pairing_status", "unknown_error")
            if pairing_status in analysis["en_pairing_stats"]["converted_samples_pairing"]:
                analysis["en_pairing_stats"]["converted_samples_pairing"][pairing_status] += 1
            
            # åˆ†æç‰¹æ®Štoken
            special_analysis = sample.get("special_token_analysis", {})
            assistant_analysis = special_analysis.get("assistant_response", {})
            
            # ç»Ÿè®¡åŒ…å«ç‰¹æ®Štokençš„æ ·æœ¬
            if assistant_analysis.get("total_special_tokens", 0) > 0:
                analysis["special_token_stats"]["samples_with_tokens"] += 1
                analysis["special_token_stats"]["total_token_instances"] += assistant_analysis["total_special_tokens"]
                
                # ç»Ÿè®¡tokenç±»å‹åˆ†å¸ƒ
                token_stats = assistant_analysis.get("token_stats", {})
                for token_type, tokens in token_stats.items():
                    if token_type not in analysis["special_token_stats"]["token_type_distribution"]:
                        analysis["special_token_stats"]["token_type_distribution"][token_type] = 0
                    
                    for token, count in tokens.items():
                        analysis["special_token_stats"]["token_type_distribution"][token_type] += count
                        
                        if token not in analysis["special_token_stats"]["specific_token_counts"]:
                            analysis["special_token_stats"]["specific_token_counts"][token] = 0
                        analysis["special_token_stats"]["specific_token_counts"][token] += count
                
                # ç»Ÿè®¡å¹³è¡¡é—®é¢˜
                balance_check = assistant_analysis.get("balance_check", {})
                for pair_name, pair_info in balance_check.items():
                    if not pair_info.get("balanced", True):
                        analysis["special_token_stats"]["balance_issues"]["total_balance_issues"] += 1
                        if pair_name not in analysis["special_token_stats"]["balance_issues"]["issue_details"]:
                            analysis["special_token_stats"]["balance_issues"]["issue_details"][pair_name] = 0
                        analysis["special_token_stats"]["balance_issues"]["issue_details"][pair_name] += 1
            
            for segment in segments:
                seg_type = segment["segment_type"]
                analysis["segment_types"][seg_type] = analysis["segment_types"].get(seg_type, 0) + 1
                
                weight = segment["weight"]
                weight_key = f"weight_{weight}"
                analysis["weight_distribution"][weight_key] = analysis["weight_distribution"].get(weight_key, 0) + 1
                
                if segment["learn"]:
                    analysis["learning_segments"] += 1
                    analysis["content_segments"] += 1
                else:
                    if seg_type in ["system_prompt", "user_query", "assistant_start_tag"]:
                        analysis["template_overhead"] += 1
                
                # ç»Ÿè®¡chinese_text
                if seg_type == "chinese_text":
                    analysis["chinese_text_stats"]["total_chinese_text_segments"] += 1
                    if segment["learn"]:
                        analysis["chinese_text_stats"]["learning_chinese_text_segments"] += 1
                    if 'original_learn' in segment and segment['original_learn'] != segment['learn']:
                        analysis["chinese_text_stats"]["modified_chinese_text_segments"] += 1
                
                # ç»Ÿè®¡æ®µè½çº§åˆ«çš„ç‰¹æ®Štoken
                if 'special_tokens' in segment and segment['special_tokens'] is not None:
                    tokens = segment['special_tokens']
                    if isinstance(tokens, dict) and tokens.get('total_special_tokens', 0) > 0:
                        analysis["special_token_stats"]["segments_with_tokens"] += 1
        
        # è®¡ç®—æ¯”ä¾‹
        analysis["learning_ratio"] = analysis["learning_segments"] / analysis["total_segments"] if analysis["total_segments"] > 0 else 0
        analysis["template_ratio"] = analysis["template_overhead"] / analysis["total_segments"] if analysis["total_segments"] > 0 else 0
        
        # chinese_textç»Ÿè®¡æ¯”ä¾‹
        chinese_stats = analysis["chinese_text_stats"]
        if chinese_stats["total_chinese_text_segments"] > 0:
            chinese_stats["learning_ratio"] = chinese_stats["learning_chinese_text_segments"] / chinese_stats["total_chinese_text_segments"]
            chinese_stats["modification_ratio"] = chinese_stats["modified_chinese_text_segments"] / chinese_stats["total_chinese_text_segments"]
        else:
            chinese_stats["learning_ratio"] = 0
            chinese_stats["modification_ratio"] = 0
        
        # ç‰¹æ®Štokenç»Ÿè®¡æ¯”ä¾‹
        special_stats = analysis["special_token_stats"]
        if analysis["total_samples"] > 0:
            special_stats["token_coverage_ratio"] = special_stats["samples_with_tokens"] / analysis["total_samples"]
        else:
            special_stats["token_coverage_ratio"] = 0
            
        if analysis["total_segments"] > 0:
            special_stats["segment_token_ratio"] = special_stats["segments_with_tokens"] / analysis["total_segments"]
        else:
            special_stats["segment_token_ratio"] = 0
        
        # è®¡ç®—è¿‡æ»¤æ•ˆç‡
        if pairing_stats:
            original_total = sum(pairing_stats.values())
            if original_total > 0:
                analysis["en_pairing_stats"]["filtering_efficiency"] = len(samples) / original_total
        
        return analysis
    
    def save_converted_data(self, samples: List[Dict[str, Any]], output_dir: str):
        """ä¿å­˜è½¬æ¢åçš„æ•°æ®"""
        
        os.makedirs(output_dir, exist_ok=True)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(samples)
        
        # æŒ‰90/10æ¯”ä¾‹åˆ†å‰²
        train_size = int(0.8 * len(df))
        train_df = df[:train_size]
        test_df = df[train_size:]
        
        # ä¿å­˜parquetæ–‡ä»¶
        train_path = os.path.join(output_dir, "qwen25_train.parquet")
        test_path = os.path.join(output_dir, "qwen25_test.parquet")
        
        train_df.to_parquet(train_path, index=False)
        test_df.to_parquet(test_path, index=False)
        
        # ä¿å­˜è½¬æ¢é…ç½®
        config = {
            "template": "qwen25",
            "system_prompt": self.system_prompt,
            "user_query": self.user_query,
            "template_tokens": self.qwen_template,
            "special_tokens": list(self.special_tokens.keys()),
            "conversion_info": {
                "original_format": "structured",
                "target_format": "qwen25_structured",
                "loss_strategy": "modified_assistant_only",
                "description": "System prompt, user queryå’Œtemplate tokensä¸è®¡ç®—lossï¼Œassistantå†…å®¹éƒ¨åˆ†(åŒ…æ‹¬chinese_text)æŒ‰æƒé‡è®¡ç®—loss",
                "chinese_text_modification": {
                    "enabled": True,
                    "description": "chinese_textæ®µè½å¼ºåˆ¶è®¾ç½®ä¸ºå­¦ä¹ çŠ¶æ€ï¼Œæƒé‡è®¾ä¸º1.0(å¦‚æœåŸæ¥æ˜¯0.0)",
                    "reason": "ç¡®ä¿ä¸­æ–‡å†…å®¹ä¹Ÿå‚ä¸æ¨¡å‹è®­ç»ƒ"
                },
                "special_token_analysis": {
                    "enabled": True,
                    "description": "åˆ†æå¹¶ç»Ÿè®¡æ‰€æœ‰ç‰¹æ®Štokençš„ä½¿ç”¨æƒ…å†µ",
                    "tracked_tokens": self.special_tokens
                },
                "data_filtering": {
                    "enabled": True,
                    "description": "è¿‡æ»¤ç‰¹æ®Štokenä¸å¹³è¡¡æˆ–æ•°é‡å¼‚å¸¸çš„æ ·æœ¬",
                    "rules": {
                        "<EN>_</EN>": "åªæ¥å—æ°å¥½1å¯¹æˆ–0å¯¹çš„æ ·æœ¬",
                        "other_pairs": "æ£€æŸ¥å¹³è¡¡æ€§"
                    }
                },
                "en_pairing_analysis": {  # æ–°å¢ï¼šENé…å¯¹åˆ†æé…ç½®
                    "enabled": True,
                    "description": "è¯¦ç»†åˆ†ææ¯ä¸ªæ ·æœ¬ä¸­<EN>å’Œ</EN>çš„é…å¯¹æƒ…å†µ",
                    "filtering_criteria": "åªä¿ç•™æ— tokenæˆ–æ°å¥½1å¯¹EN tokençš„æ ·æœ¬"
                }
            }
        }
        
        config_path = os.path.join(output_dir, "qwen25_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ•°æ®åˆ†æ
        analysis = self.analyze_converted_data(samples)
        analysis_path = os.path.join(output_dir, "qwen25_analysis.json")
        
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        return train_path, test_path, config_path, analysis_path


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Qwen2.5 æ•°æ®æ ¼å¼è½¬æ¢å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰")
    print("=" * 60)
    print("åŠŸèƒ½ï¼šå°†åŸå§‹å°è¯´æ•°æ®è½¬æ¢ä¸ºQwen2.5æ¨¡æ¿æ ¼å¼")
    print("ğŸ”¥ ç‰¹æ€§ï¼šchinese_text éƒ¨åˆ†å‚ä¸ loss è®¡ç®—")
    print("ğŸ¯ æ–°åŠŸèƒ½ï¼šç‰¹æ®ŠTokenç»Ÿè®¡åˆ†æå’Œæ•°æ®è¿‡æ»¤")
    print("ğŸ” é‡ç‚¹åŠŸèƒ½ï¼š<EN></EN>é…å¯¹è¯¦ç»†ç»Ÿè®¡")
    print("=" * 60)
    
    # è¾“å…¥è¾“å‡ºè·¯å¾„
    input_dir = "/home/jovyan/fdu_new/zyn/examples/data/story"
    output_dir = "/home/jovyan/fdu_new/zyn/examples/data/qwen25_format"
    
    # åˆ›å»ºè½¬æ¢å™¨
    converter = Qwen25DataConverter()
    
    # å­˜å‚¨æ‰€æœ‰è½¬æ¢åçš„æ ·æœ¬ç”¨äºæœ€ç»ˆåˆ†æ
    all_converted_samples = []
    all_pairing_stats = {
        "no_tokens": 0,
        "perfect_pair": 0,
        "multiple_balanced": 0,
        "excess_starts": 0,
        "excess_ends": 0,
        "unknown_error": 0
    }
    
    # å¤„ç†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    for dataset_type in ["train", "test"]:
        print(f"\nğŸ“‚ å¤„ç† {dataset_type} æ•°æ®é›†")
        print("-" * 40)
        
        # åŠ è½½åŸå§‹æ•°æ®
        input_path = os.path.join(input_dir, f"{dataset_type}.parquet")
        if not os.path.exists(input_path):
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
            continue
            
        original_samples = converter.load_original_data(input_path)
        
        # è½¬æ¢æ•°æ®ï¼ˆåŒ…å«è¿‡æ»¤å’Œé…å¯¹ç»Ÿè®¡ï¼‰
        converted_samples, pairing_stats = converter.convert_batch(original_samples)
        all_converted_samples.extend(converted_samples)
        
        # ç´¯åŠ é…å¯¹ç»Ÿè®¡
        for key in all_pairing_stats:
            all_pairing_stats[key] += pairing_stats[key]
        
        # ä¿å­˜è½¬æ¢ç»“æœ
        output_path = os.path.join(output_dir, f"qwen25_{dataset_type}.parquet")
        os.makedirs(output_dir, exist_ok=True)
        
        df = pd.DataFrame(converted_samples)
        df.to_parquet(output_path, index=False)
        print(f"âœ… ä¿å­˜å®Œæˆ: {output_path}")
    
    if all_converted_samples:
        # ä¿å­˜é…ç½®å’Œåˆ†æ
        train_path, test_path, config_path, analysis_path = converter.save_converted_data(
            all_converted_samples, output_dir
        )
        
        # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡ï¼ˆåŒ…å«é…å¯¹ç»Ÿè®¡ï¼‰
        analysis = converter.analyze_converted_data(all_converted_samples, all_pairing_stats)
        
        print(f"\nğŸ“Š è½¬æ¢å®Œæˆç»Ÿè®¡:")
        print("=" * 60)
        print(f"æ€»æ ·æœ¬æ•°: {analysis['total_samples']}")
        print(f"å­¦ä¹ æ®µè½æ¯”ä¾‹: {analysis['learning_ratio']:.2%}")
        print(f"chinese_textå­¦ä¹ æ¯”ä¾‹: {analysis['chinese_text_stats']['learning_ratio']:.2%}")
        print(f"ç‰¹æ®Štokenè¦†ç›–ç‡: {analysis['special_token_stats']['token_coverage_ratio']:.2%}")
        print(f"å¹³è¡¡é—®é¢˜æ•°é‡: {analysis['special_token_stats']['balance_issues']['total_balance_issues']}")
        
        # æ˜¾ç¤ºENé…å¯¹ç»Ÿè®¡
        print(f"\nğŸ” <EN></EN>é…å¯¹æœ€ç»ˆç»Ÿè®¡:")
        print("-" * 40)
        original_pairing = analysis["en_pairing_stats"]["original_samples_pairing"]
        converted_pairing = analysis["en_pairing_stats"]["converted_samples_pairing"]
        
        if original_pairing:
            original_total = sum(original_pairing.values())
            print(f"ğŸ“ˆ åŸå§‹æ•°æ®é…å¯¹åˆ†å¸ƒ:")
            for status, count in original_pairing.items():
                percentage = count / original_total * 100 if original_total > 0 else 0
                print(f"  {status}: {count} ({percentage:.1f}%)")
            
            print(f"\nğŸ“Š è½¬æ¢åæ•°æ®é…å¯¹åˆ†å¸ƒ:")
            converted_total = sum(converted_pairing.values())
            for status, count in converted_pairing.items():
                percentage = count / converted_total * 100 if converted_total > 0 else 0
                print(f"  {status}: {count} ({percentage:.1f}%)")
            
            filtering_efficiency = analysis["en_pairing_stats"]["filtering_efficiency"]
            print(f"\nâœ… è¿‡æ»¤æ•ˆç‡: {filtering_efficiency:.2%} ({converted_total}/{original_total})")
        
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"è®­ç»ƒé›†: {train_path}")
        print(f"æµ‹è¯•é›†: {test_path}")
        print(f"é…ç½®æ–‡ä»¶: {config_path}")
        print(f"åˆ†ææ–‡ä»¶: {analysis_path}")
        print("=" * 60)


if __name__ == "__main__":
    main()