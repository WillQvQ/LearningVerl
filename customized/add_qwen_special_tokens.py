#!/usr/bin/env python3
"""
ä¸º Qwen2.5-3B-Instruct æ¨¡å‹æ·»åŠ  special tokens '<EN>' å’Œ '</EN>'
åŸºäº LearningVerl issue #9 çš„å®æ–½æ–¹æ¡ˆ
"""

import json
import os
import shutil
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def check_model_status(model_path):
    """æ£€æŸ¥ Qwen æ¨¡å‹çš„è¯æ±‡è¡¨çŠ¶æ€"""
    print("ğŸ” æ£€æŸ¥æ¨¡å‹å½“å‰çŠ¶æ€...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path, 
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="cpu"  # åªåŠ è½½åˆ°CPUæ£€æŸ¥
    )
    
    vocab_size = len(tokenizer)
    embedding_size = model.get_input_embeddings().weight.shape[0]
    lm_head_size = model.lm_head.weight.shape[0]
    available_slots = embedding_size - vocab_size
    
    print(f"ğŸ“Š æ¨¡å‹çŠ¶æ€æŠ¥å‘Š:")
    print(f"  Tokenizer è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"  æ¨¡å‹ embedding å±‚å¤§å°: {embedding_size}")
    print(f"  æ¨¡å‹ lm_head å±‚å¤§å°: {lm_head_size}")
    print(f"  å¯ç”¨çš„é¢„ç•™ token ä½ç½®: {available_slots}")
    
    # æ£€æŸ¥ä¸€äº›é«˜IDçš„tokens
    print(f"\nğŸ”¢ é«˜IDä½ç½®çš„tokens:")
    for i in range(max(0, vocab_size-5), min(vocab_size+5, embedding_size)):
        try:
            token = tokenizer.decode([i])
            print(f"  ID {i}: '{token}'")
        except:
            print(f"  ID {i}: <æœªå®šä¹‰>")
    
    return vocab_size, embedding_size, available_slots

def add_special_tokens_to_qwen(model_path, new_tokens, output_path=None):
    """
    ä¸º Qwen æ¨¡å‹æ·»åŠ  special tokens
    
    Args:
        model_path: åŸå§‹æ¨¡å‹è·¯å¾„
        new_tokens: è¦æ·»åŠ çš„ special tokens åˆ—è¡¨
        output_path: è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™å°±åœ°ä¿®æ”¹
    """
    if output_path is None:
        output_path = model_path
    
    print(f"ğŸ”§ ä¸ºæ¨¡å‹æ·»åŠ ç‰¹æ®Štokens: {new_tokens}")
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    original_vocab_size = len(tokenizer)
    print(f"  åŸå§‹è¯æ±‡è¡¨å¤§å°: {original_vocab_size}")
    
    # æ·»åŠ æ–°çš„ special tokens
    print(f"  æ·»åŠ  special tokens: {new_tokens}")
    
    # ä½¿ç”¨ add_tokens æ–¹æ³•æ·»åŠ ç‰¹æ®Štoken
    num_added = tokenizer.add_tokens(new_tokens, special_tokens=True)
    print(f"  æˆåŠŸæ·»åŠ  {num_added} ä¸ª special tokens")
    
    new_vocab_size = len(tokenizer)
    print(f"  æ–°è¯æ±‡è¡¨å¤§å°: {new_vocab_size}")
    
    # éªŒè¯æ–° tokens
    print(f"\nâœ… æ–° tokens çš„ ID:")
    token_ids = {}
    for token in new_tokens:
        token_id = tokenizer.convert_tokens_to_ids(token)
        token_ids[token] = token_id
        print(f"    {token}: {token_id}")
    
    # ä¿å­˜ä¿®æ”¹åçš„ tokenizer
    print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_path}")
    tokenizer.save_pretrained(output_path)
    
    return tokenizer, token_ids

def test_special_tokens(model_path, special_tokens):
    """æµ‹è¯•ç‰¹æ®Štokensæ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print(f"ğŸ§ª æµ‹è¯•ç‰¹æ®Štokens...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    # æµ‹è¯•ç¼–ç è§£ç 
    test_text = f"è¿™æ˜¯ä¸­æ–‡å†…å®¹ {special_tokens[0]}This is English content{special_tokens[1]} è¿™æ˜¯ä¸­æ–‡æ‘˜è¦"
    
    print(f"  æµ‹è¯•æ–‡æœ¬: {test_text}")
    
    # ç¼–ç 
    encoded = tokenizer.encode(test_text, add_special_tokens=False)
    print(f"  ç¼–ç ç»“æœ: {encoded}")
    
    # è§£ç 
    decoded = tokenizer.decode(encoded)
    print(f"  è§£ç ç»“æœ: {decoded}")
    
    # éªŒè¯ç‰¹æ®Štokens
    for token in special_tokens:
        token_id = tokenizer.convert_tokens_to_ids(token)
        decoded_token = tokenizer.decode([token_id])
        print(f"  {token} -> ID: {token_id} -> è§£ç : '{decoded_token}'")
    
    return encoded, decoded

def update_convert_script_for_special_tokens(examples_dir):
    """æ›´æ–°è½¬æ¢è„šæœ¬ä»¥æ”¯æŒç‰¹æ®Štokens"""
    print(f"ğŸ“ æ›´æ–°æ•°æ®è½¬æ¢è„šæœ¬...")
    
    # è¯»å–ç°æœ‰çš„è½¬æ¢è„šæœ¬
    convert_script_path = os.path.join(examples_dir, "convert_to_qwen25.py")
    
    if os.path.exists(convert_script_path):
        print(f"  å‘ç°ç°æœ‰è½¬æ¢è„šæœ¬: {convert_script_path}")
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´æ–°é€»è¾‘
    else:
        print(f"  åˆ›å»ºæ–°çš„è½¬æ¢è„šæœ¬: {convert_script_path}")
        # åˆ›å»ºæ”¯æŒç‰¹æ®Štokençš„è½¬æ¢è„šæœ¬
        create_enhanced_convert_script(convert_script_path)

def create_enhanced_convert_script(script_path):
    """åˆ›å»ºå¢å¼ºçš„æ•°æ®è½¬æ¢è„šæœ¬"""
    script_content = '''#!/usr/bin/env python3
"""
Qwen2.5 æ•°æ®æ ¼å¼è½¬æ¢å™¨ï¼ˆæ”¯æŒç‰¹æ®Štoken '<EN>' å’Œ '</EN>'ï¼‰
åŸºäº LearningVerl çš„å®ç°æ–¹æ¡ˆ
"""

import os
import json
import pandas as pd
from typing import List, Dict, Any
from tqdm import tqdm
from transformers import AutoTokenizer

class Qwen25DataConverterWithENTokens:
    """Qwen2.5æ•°æ®æ ¼å¼è½¬æ¢å™¨ï¼ˆæ”¯æŒ <EN> </EN> ç‰¹æ®Štokenï¼‰"""
    
    def __init__(self, model_path: str):
        # Qwen2.5å®˜æ–¹system prompt
        self.system_prompt = "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."
        
        # å›ºå®šçš„ç”¨æˆ·query
        self.user_query = "è¯·ä½ å¸®æˆ‘å†™ä¸€ä¸ª 4000 å­—çš„å°è¯´ï¼Œå¹¶ç¿»è¯‘æˆè‹±æ–‡"
        
        # ç‰¹æ®Štokenå®šä¹‰ï¼ˆä¸åŸæ•°æ®é›†ä¿æŒä¸€è‡´ï¼‰
        self.special_tokens = ["<EN>", "</EN>"]
        
        # Qwen2.5æ¨¡æ¿æ ¼å¼
        self.qwen_template = {
            "system_start": "<|im_start|>system\\n",
            "system_end": "<|im_end|>\\n",
            "user_start": "<|im_start|>user\\n", 
            "user_end": "<|im_end|>\\n",
            "assistant_start": "<|im_start|>assistant\\n",
            "assistant_end": "<|im_end|>"
        }
        
        # åŠ è½½å·²æ›´æ–°çš„tokenizer
        self.model_path = model_path
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        # éªŒè¯ç‰¹æ®Štokens
        self._verify_special_tokens()
    
    def _verify_special_tokens(self):
        """éªŒè¯ç‰¹æ®Štokensæ˜¯å¦å·²æ­£ç¡®æ·»åŠ """
        print("ğŸ” éªŒè¯ç‰¹æ®Štokens...")
        
        for token in self.special_tokens:
            token_id = self.tokenizer.convert_tokens_to_ids(token)
            if token_id == self.tokenizer.unk_token_id:
                raise ValueError(f"ç‰¹æ®Štoken {token} æœªæ‰¾åˆ°ï¼è¯·å…ˆè¿è¡Œ add_qwen_special_tokens.py")
            print(f"  âœ… {token}: {token_id}")
    
    def load_original_data(self, data_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½åŸå§‹æ•°æ®"""
        print(f"ğŸ“– åŠ è½½åŸå§‹æ•°æ®: {data_path}")
        
        if data_path.endswith('.parquet'):
            df = pd.read_parquet(data_path)
            data = df.to_dict('records')
        else:
            raise ValueError("æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .parquet")
        
        print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(data)} ä¸ªæ ·æœ¬")
        return data
    
    def extract_assistant_response(self, sample: Dict[str, Any]) -> str:
        """ä»åŸå§‹æ ·æœ¬ä¸­æå–åŠ©æ‰‹å›å¤å†…å®¹"""
        segments = sample["data"]["segments"]
        
        response_parts = []
        for segment in segments:
            content = segment["content"]
            response_parts.append(content)
        
        # ç”¨æ¢è¡Œç¬¦è¿æ¥æ‰€æœ‰éƒ¨åˆ†
        assistant_response = "\\n\\n".join(response_parts)
        return assistant_response
    
    def analyze_special_tokens(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬ä¸­çš„ç‰¹æ®Štokenä½¿ç”¨æƒ…å†µ"""
        analysis = {
            "has_en_tokens": False,
            "en_start_count": 0,
            "en_end_count": 0,
            "balanced": False,
            "en_segments": []
        }
        
        # ç»Ÿè®¡ç‰¹æ®Štoken
        analysis["en_start_count"] = text.count("<EN>")
        analysis["en_end_count"] = text.count("</EN>")
        analysis["has_en_tokens"] = analysis["en_start_count"] > 0 or analysis["en_end_count"] > 0
        analysis["balanced"] = analysis["en_start_count"] == analysis["en_end_count"]
        
        # æå–è‹±æ–‡æ®µè½
        if analysis["balanced"] and analysis["en_start_count"] > 0:
            import re
            en_pattern = r'<EN>(.*?)</EN>'
            matches = re.findall(en_pattern, text, re.DOTALL)
            analysis["en_segments"] = matches
        
        return analysis
    
    def create_qwen25_conversation(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºQwen2.5æ ¼å¼çš„å¯¹è¯"""
        
        # æå–åŠ©æ‰‹å›å¤
        assistant_response = self.extract_assistant_response(sample)
        
        # åˆ†æç‰¹æ®Štoken
        en_token_analysis = self.analyze_special_tokens(assistant_response)
        
        # æ„å»ºå®Œæ•´çš„å¯¹è¯æ–‡æœ¬
        conversation = (
            f"{self.qwen_template['system_start']}"
            f"{self.system_prompt}"
            f"{self.qwen_template['system_end']}"
            f"{self.qwen_template['user_start']}"
            f"{self.user_query}"
            f"{self.qwen_template['user_end']}"
            f"{self.qwen_template['assistant_start']}"
            f"{assistant_response}"
            f"{self.qwen_template['assistant_end']}"
        )
        
        # åˆ›å»ºloss mask segments
        segments = sample["data"]["segments"]
        loss_segments = []
        
        # Systeméƒ¨åˆ† - ä¸è®¡ç®—loss
        system_text = (
            f"{self.qwen_template['system_start']}"
            f"{self.system_prompt}"
            f"{self.qwen_template['system_end']}"
        )
        loss_segments.append({
            "content": system_text,
            "learn": False,
            "weight": 0.0,
            "segment_type": "system_prompt",
            "segment_id": 0
        })
        
        # Useréƒ¨åˆ† - ä¸è®¡ç®—loss
        user_text = (
            f"{self.qwen_template['user_start']}"
            f"{self.user_query}"
            f"{self.qwen_template['user_end']}"
        )
        loss_segments.append({
            "content": user_text,
            "learn": False,
            "weight": 0.0,
            "segment_type": "user_query",
            "segment_id": 1
        })
        
        # Assistantå¼€å§‹æ ‡ç­¾ - ä¸è®¡ç®—loss
        loss_segments.append({
            "content": self.qwen_template['assistant_start'],
            "learn": False,
            "weight": 0.0,
            "segment_type": "assistant_start_tag",
            "segment_id": 2
        })
        
        # Assistantå†…å®¹éƒ¨åˆ† - ä¿æŒåŸæœ‰lossæƒé‡
        segment_id = 3
        for segment in segments:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹æ®Štoken
            is_special_token = segment["content"] in self.special_tokens
            
            loss_segments.append({
                "content": segment["content"],
                "learn": segment["learn"],
                "weight": segment["weight"],
                "segment_type": segment["segment_type"],
                "segment_id": segment_id,
                "is_special_token": is_special_token
            })
            segment_id += 1
        
        # Assistantç»“æŸæ ‡ç­¾ - ä¸è®¡ç®—loss
        loss_segments.append({
            "content": self.qwen_template['assistant_end'],
            "learn": False,
            "weight": 0.0,
            "segment_type": "assistant_end_tag",
            "segment_id": segment_id
        })
        
        return {
            "format": "qwen25_structured_with_en_tokens",
            "conversation": conversation,
            "data": {"segments": loss_segments},
            "id": sample["id"],
            "topic": sample["topic"],
            "template": "qwen25",
            "total_segments": len(loss_segments),
            "learning_segments": len([s for s in loss_segments if s["learn"]]),
            "original_segments": sample["total_segments"],
            "en_token_analysis": en_token_analysis
        }
    
    def convert_batch(self, samples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡è½¬æ¢æ ·æœ¬"""
        print(f"ğŸ”„ å¼€å§‹è½¬æ¢ {len(samples)} ä¸ªæ ·æœ¬ä¸ºQwen2.5æ ¼å¼...")
        
        converted_samples = []
        en_token_stats = {
            "samples_with_en_tokens": 0,
            "balanced_samples": 0,
            "total_en_segments": 0
        }
        
        for sample in tqdm(samples, desc="è½¬æ¢è¿›åº¦"):
            try:
                qwen_sample = self.create_qwen25_conversation(sample)
                converted_samples.append(qwen_sample)
                
                # ç»Ÿè®¡EN tokenä½¿ç”¨æƒ…å†µ
                en_analysis = qwen_sample["en_token_analysis"]
                if en_analysis["has_en_tokens"]:
                    en_token_stats["samples_with_en_tokens"] += 1
                    en_token_stats["total_en_segments"] += len(en_analysis["en_segments"])
                    if en_analysis["balanced"]:
                        en_token_stats["balanced_samples"] += 1
                        
            except Exception as e:
                print(f"âš ï¸  è½¬æ¢æ ·æœ¬ {sample.get('id', 'unknown')} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"âœ… è½¬æ¢å®Œæˆï¼Œå…± {len(converted_samples)} ä¸ªæ ·æœ¬")
        print(f"ğŸ“Š EN tokenç»Ÿè®¡:")
        print(f"  åŒ…å«EN tokençš„æ ·æœ¬: {en_token_stats['samples_with_en_tokens']}")
        print(f"  å¹³è¡¡çš„æ ·æœ¬æ•°: {en_token_stats['balanced_samples']}")
        print(f"  è‹±æ–‡æ®µè½æ€»æ•°: {en_token_stats['total_en_segments']}")
        
        return converted_samples
    
    def save_converted_data(self, samples: List[Dict[str, Any]], output_dir: str):
        """ä¿å­˜è½¬æ¢åçš„æ•°æ®"""
        os.makedirs(output_dir, exist_ok=True)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(samples)
        
        # æŒ‰80/20æ¯”ä¾‹åˆ†å‰²
        train_size = int(0.8 * len(df))
        train_df = df[:train_size]
        test_df = df[train_size:]
        
        # ä¿å­˜parquetæ–‡ä»¶
        train_path = os.path.join(output_dir, "qwen25_train.parquet")
        test_path = os.path.join(output_dir, "qwen25_test.parquet")
        
        train_df.to_parquet(train_path, index=False)
        test_df.to_parquet(test_path, index=False)
        
        # ä¿å­˜é…ç½®
        config = {
            "template": "qwen25",
            "system_prompt": self.system_prompt,
            "user_query": self.user_query,
            "template_tokens": self.qwen_template,
            "special_tokens": self.special_tokens,
            "model_path": self.model_path,
            "conversion_info": {
                "format": "qwen25_structured_with_en_tokens",
                "loss_strategy": "template_excluded_content_weighted",
                "en_token_strategy": "preserved_with_original_weights"
            }
        }
        
        config_path = os.path.join(output_dir, "qwen25_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        return train_path, test_path, config_path

def main():
    print("ğŸš€ Qwen2.5æ•°æ®è½¬æ¢ï¼ˆæ”¯æŒENç‰¹æ®Štokenï¼‰")
    print("=" * 60)
    
    # è·¯å¾„é…ç½®
    model_path = "/home/jovyan/fdu_new/zyn/ckpts/Qwen2.5-3B-Instruct"
    input_dir = "/home/jovyan/fdu_new/zyn/examples"
    output_dir = "/home/jovyan/fdu_new/zyn/examples/qwen25_format"
    
    # åˆ›å»ºè½¬æ¢å™¨
    converter = Qwen25DataConverterWithENTokens(model_path)
    
    # å¤„ç†æ•°æ®
    all_samples = []
    for dataset_type in ["train", "test"]:
        input_path = os.path.join(input_dir, f"{dataset_type}.parquet")
        if os.path.exists(input_path):
            samples = converter.load_original_data(input_path)
            converted_samples = converter.convert_batch(samples)
            all_samples.extend(converted_samples)
    
    # ä¿å­˜ç»“æœ
    if all_samples:
        train_path, test_path, config_path = converter.save_converted_data(all_samples, output_dir)
        print(f"\\nâœ… è½¬æ¢å®Œæˆï¼")
        print(f"  è®­ç»ƒé›†: {train_path}")
        print(f"  æµ‹è¯•é›†: {test_path}")
        print(f"  é…ç½®: {config_path}")

if __name__ == "__main__":
    main()
'''
    
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    os.chmod(script_path, 0o755)
    print(f"âœ… åˆ›å»ºè½¬æ¢è„šæœ¬: {script_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Qwen2.5-3B-Instruct ç‰¹æ®ŠTokenæ·»åŠ å·¥å…·")
    print("=" * 60)
    print("åŠŸèƒ½ï¼šä¸ºæ¨¡å‹æ·»åŠ  '<EN>' å’Œ '</EN>' ç‰¹æ®Štokens")
    print("åŸºäºï¼šLearningVerl issue #9 çš„å®æ–½æ–¹æ¡ˆ")
    print("=" * 60)
    
    # é…ç½®è·¯å¾„
    model_path = "/home/jovyan/fdu_new/zyn/ckpts/Qwen2.5-3B-Instruct"
    backup_path = f"{model_path}_backup_tokenizer"
    examples_dir = "/home/jovyan/fdu_new/zyn/examples"
    
    # è¦æ·»åŠ çš„ç‰¹æ®Štokens
    special_tokens = ["<EN>", "</EN>"]
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    print(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {model_path}")
    
    # æ­¥éª¤1: æ£€æŸ¥æ¨¡å‹å½“å‰çŠ¶æ€
    print(f"\n" + "="*50)
    print("æ­¥éª¤ 1: æ£€æŸ¥æ¨¡å‹çŠ¶æ€")
    print("="*50)
    
    vocab_size, embedding_size, available_slots = check_model_status(model_path)
    
    if available_slots < len(special_tokens):
        print(f"âŒ å¯ç”¨tokenä½ç½®ä¸è¶³ï¼éœ€è¦ {len(special_tokens)} ä¸ªï¼Œä»…æœ‰ {available_slots} ä¸ª")
        return
    
    # æ­¥éª¤2: åˆ›å»ºå¤‡ä»½
    print(f"\n" + "="*50)
    print("æ­¥éª¤ 2: åˆ›å»ºå¤‡ä»½")
    print("="*50)
    
    if not os.path.exists(backup_path):
        print(f"ğŸ“‹ åˆ›å»ºtokenizerå¤‡ä»½: {backup_path}")
        os.makedirs(backup_path, exist_ok=True)
        
        # å¤‡ä»½tokenizerç›¸å…³æ–‡ä»¶
        tokenizer_files = [
            "tokenizer.json", "tokenizer_config.json", 
            "vocab.json", "merges.txt", "special_tokens_map.json"
        ]
        
        for file in tokenizer_files:
            src = os.path.join(model_path, file)
            dst = os.path.join(backup_path, file)
            if os.path.exists(src):
                shutil.copy2(src, dst)
                print(f"  âœ… å¤‡ä»½: {file}")
    else:
        print(f"ğŸ“‹ å¤‡ä»½å·²å­˜åœ¨: {backup_path}")
    
    # æ­¥éª¤3: æ·»åŠ ç‰¹æ®Štokens
    print(f"\n" + "="*50)
    print("æ­¥éª¤ 3: æ·»åŠ ç‰¹æ®Štokens")
    print("="*50)
    
    try:
        tokenizer, token_ids = add_special_tokens_to_qwen(model_path, special_tokens)
        print(f"âœ… ç‰¹æ®Štokensæ·»åŠ æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ æ·»åŠ ç‰¹æ®Štokenså¤±è´¥: {e}")
        return
    
    # æ­¥éª¤4: æµ‹è¯•éªŒè¯
    print(f"\n" + "="*50)
    print("æ­¥éª¤ 4: æµ‹è¯•éªŒè¯")
    print("="*50)
    
    try:
        encoded, decoded = test_special_tokens(model_path, special_tokens)
        print(f"âœ… ç‰¹æ®Štokensæµ‹è¯•é€šè¿‡ï¼")
    except Exception as e:
        print(f"âŒ ç‰¹æ®Štokensæµ‹è¯•å¤±è´¥: {e}")
        return
    
    # æ­¥éª¤5: æ›´æ–°ç›¸å…³è„šæœ¬
    print(f"\n" + "="*50)
    print("æ­¥éª¤ 5: åˆ›å»ºè½¬æ¢è„šæœ¬")
    print("="*50)
    
    update_convert_script_for_special_tokens(examples_dir)
    
    # æ€»ç»“
    print(f"\n" + "="*60)
    print("ğŸ‰ ç‰¹æ®ŠTokenæ·»åŠ å®Œæˆï¼")
    print("="*60)
    print(f"âœ… å·²æ·»åŠ ç‰¹æ®Štokens: {special_tokens}")
    print(f"ğŸ“Š Token IDæ˜ å°„:")
    for token, token_id in token_ids.items():
        print(f"    {token}: {token_id}")
    
    print(f"\nğŸ“ ç›¸å…³æ–‡ä»¶:")
    print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"  å¤‡ä»½è·¯å¾„: {backup_path}")
    
    print(f"\nğŸ“‹ åç»­æ­¥éª¤:")
    print(f"  1. è¿è¡Œæ•°æ®è½¬æ¢è„šæœ¬è¿›è¡Œæ ¼å¼è½¬æ¢")
    print(f"  2. ä½¿ç”¨æ›´æ–°åçš„æ¨¡å‹è¿›è¡Œè®­ç»ƒ")
    print(f"  3. å¦‚éœ€æ¢å¤ï¼Œä»å¤‡ä»½è·¯å¾„å¤åˆ¶æ–‡ä»¶")
    
    print(f"\nğŸ’¡ é›†æˆåˆ°verlè®­ç»ƒ:")
    print(f"  ä½¿ç”¨æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"  ç‰¹æ®Štokenså·²è‡ªåŠ¨è¯†åˆ«ï¼Œæ— éœ€é¢å¤–é…ç½®")
    
    print("="*60)

if __name__ == "__main__":
    main()
